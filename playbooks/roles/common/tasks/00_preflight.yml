
- name: Check if remote-sensors is defined
  fail:
    msg: 'Verify you have the remote-sensors group in your inventory file. It must be defined. See sample inventory. If you have no remote sensors you can define an empty group with just remote-senors: under nodes.'
  when: "'remote-sensors' not in groups"

#The below code ssh's to every node from every node.
#the -oBatchMode=yes tells SSH to fail if prompted for a password
#without that option the ssh terminal will hang if it cannot connect via keys
- name: Verify all nodes can SSH to each other with keys not passwords
  shell: "ssh -oBatchMode=yes root@{{ item }}"
  with_items: "{{ groups['nodes'] }}"
  changed_when: false

#The below code verifies that the management IPV4 addresses are valid IPs
#Meaning that they have 4 octets and only contain numbers in those octets
- name: Verify IP scheme (111.111.111.111)
  fail:
    msg: "{{ item }} is not a valid IP"
  when: (item | regex_search('\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')) != item
  with_items: "{{ management_ipv4 }}"
  changed_when: False

#The below code verifies that the management IPV4 addresses are valid IP ranges
#This means that each octet ranges 0-255 nothing more, nothing less
- name: Verify valid IP range (0-255)
  fail:
    msg: "{{ item }} is not a valid IP range"
  when: item.split('.')[0] | int > 255 or item.split('.')[1] | int > 255 or item.split('.')[2] | int > 255 or item.split('.')[3] | int > 255
  with_items: "{{ management_ipv4 }}"
  changed_when: false


- name: ping all ips in inventory
  shell: "ping -c 1 {{ item }}"
  with_items: "{{ groups['nodes'] }}"
  changed_when: False

- name: ping google DNS by IP
  shell: "ping -c 1 8.8.8.8"
  changed_when: False

- name: ping www.Google.com
  shell: "ping -c 1 www.google.com"
  changed_when: False

# Checking that required variables are set
- name: Checking that required variables are set
  fail:
    msg: "{{ item }} is not defined. Make sure it is defined in your inventory file. See sample inventory for help."
  when: not item
  with_items:
    - management_ipv4
    - epel_repo_url
    - epel_repo_gpg_key_url
    - epel_repofile_path
    - home_net
    - services_cidr
    - es_mem
    - es_masters
    - pcap_bpf_filter
    - es_datas
    - elastic_pv_size
    - kafka_pv_size
    - zookeeper_pv_size
    - zookeeper_replicas
    - pcap_pv_size

# Perform Elasticsearch checks
- name: Verify Elasticsearch heap memory is less than half of physical RAM
  fail:
    msg: 'Ensure your elasticsearch memory is half your physical RAM or less. See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html'
  when: inventory_hostname in groups['elasticsearch'] and (ansible_memory_mb.real.total / 2) < es_mem * 1024

- name: Verify Elasticsearch memory does not exceed 26GB
  fail:
    msg: 'For optimization, Elasticsearch memory should not exceed 26GB. See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html'
  when: inventory_hostname in groups['elasticsearch'] and es_mem > 26

- name: Verify double es_mem is available on the machine
  fail:
    msg: "es_mem (elasticsearch memory) consumes es_mem*2 for performance reasons. Your host has {{ ansible_memory_mb.real.total * 1024 }} and you have requested {{ es_mem * 2 * 1024 }}MB for Elasticsearch."
  when: inventory_hostname in groups['elasticsearch'] and (es_mem * 2 * 1024) > ansible_memory_mb.real.total

# This grabs the drive from each machine. For example /dev/sdb would give you sdb
- name: Get the drive names
  shell: "echo {{ item.split('/')[2] }}"
  loop: "{{ hostvars[inventory_hostname].data_disk_devices }}"
  register: disk_names
  changed_when: false

# This is just an array with all the disk names
- name: Create array with correct disk name
  set_fact:
    data_disks: "{{ disk_names.results | map(attribute='stdout') | list }}"

# This will throw an error if the disk name provided isn't in the list of
# available ansible devices
- name: Verify that all devices are actually drives
  fail:
    msg: "{{ item }} is not a drive. Make sure that you provided the name of a full drive and not a partition."
  when: item not in ansible_devices
  with_items: "{{ data_disks }}"

# This next portion of the code is pure magic. I challenge someone to find
# a way to get Ansible to do this in a cleaner way.

# This grabs the size portion of each valid ansible device. In this case,
# the valid disks would be things like sda, sdb, etc (assuming those are
# the names of the disks on the actual server).  The split.(' ')[0] you see
# is because the size is a string like 30 GB and if you leave the GB in there
# it won't convert to an int correctly. The split just grabs the numerical
# portion.
- set_fact:
    disk_size: "{{ item.value.size.split(' ')[0] | int }}"
  with_dict: "{{ hostvars[inventory_hostname].ansible_devices }}"
  register: valid_disks
  when: item.key in data_disks

# This will take all the valid disks on the server and add them up. So if
# a server has two disks sda and sdb both with 30GB this would be 60.
- set_fact:
    total_size_on_server: "{{ valid_disks.results | selectattr('skipped', 'undefined') | map(attribute='ansible_facts.disk_size') | map('int') | sum(start=0) }}"

# Here is the big problem with Ansible. Each server now has a fact called
# total_size_on_server. The problem is a single server can't access that
# fact on other servers so we have to find a way to get each server that
# information IN A SINGLE LIST. This is because constraints ansible has
# with nested variables. (Like I said - I challenge you to come up with a
# better way to do this.)
- set_fact:
    total_space: "{{ hostvars[item].total_size_on_server }}"
  register: disk_results
  loop: "{{ groups['ceph'] }}"
  when: hostvars[item].total_size_on_server is defined

# Now we can use the map function (because everything is in a single dictionary)
# to sum everything in our list
- set_fact:
    total_space_available: "{{ disk_results.results | map(attribute='ansible_facts.total_space') | map('int') | sum(start=0) }}"

- debug:
    msg: "{{ total_space_available }}"

# TODO - this should include stenographer, but currently does not
- set_fact:
    total_space_required: "{{ (elastic_pv_size * ( groups['elasticsearch'] | length )) + ( kafka_pv_size * ( groups['kafka'] | length )) + ( zookeeper_pv_size * ( hostvars[inventory_hostname].zookeeper_replicas )) + ( pcap_pv_size * ( groups['moloch'] | length )) }}"

- debug:
    msg: "{{ total_space_required }}"

- name: Verify there is sufficient space for the PVs requested
  fail:
    msg: "The drives {{ data_disks }} do not have enough collective space to house the data you requested. You requested {{ total_space_required }} GB, which exceeds the {{ total_space_available }} GB you have on your drives."
  when: (total_space_available | int) < (total_space_required | int)

# The below code will verify, per host, that every drive listed in the inventory file for that host does not contain the root directory / the OS
- name: Verify the drives do not contain root
  shell: "lsblk | grep '{{ item[item.rfind('/') + 1:] }}'"
  with_items:
    - "{{ data_disk_devices }}"
  register: command_result
  failed_when:
   - "'/' in command_result.stdout"
  changed_when: False
